{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51695774902e2a3a3d29f5733b31542b6f91d5fa"
      },
      "cell_type": "code",
      "source": "# LOAD LIBRARIES\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "437f141e31c4a4e14a8cd93942353eb1671365b9"
      },
      "cell_type": "code",
      "source": "\n\n# LOAD THE DATA\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# PREPARE DATA FOR NEURAL NETWORK\nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1)\n#Normalization\nX_train = X_train / 255.0\nX_test = test / 255.0\n#Reshaping to conv.\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)\n\n# GLOBAL VARIABLES\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c11a7bc48ad8cb7deaeed7edc517f4cb88ee941c"
      },
      "cell_type": "code",
      "source": "# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 3\nmodel = [0]*3\n\nfor j in range(3):\n    model[j] = Sequential()\n    model[j].add(Conv2D(32,kernel_size=5,padding='same',activation='relu',\n            input_shape=(28,28,1)))\n    model[j].add(MaxPool2D())\n    if j>0:\n        model[j].add(Conv2D(48,kernel_size=5,padding='same',activation='relu'))\n        model[j].add(MaxPool2D())\n    if j>1:\n        model[j].add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n        model[j].add(MaxPool2D(padding='same'))\n    model[j].add(Flatten())\n    model[j].add(Dense(256, activation='relu'))\n    model[j].add(Dense(10, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e58d53aa20b07ef77878b9dbc5d61b5e613fea6",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"1\",\"2\",\"3\"]\nepochs = 20\nfor j in range(nets):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=64, epochs = epochs, \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "812c755eabd49f01d022d81e6c18e24c86bf541c"
      },
      "cell_type": "code",
      "source": "# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8165d70653b2a210231ca5148e2a39ad636909e6"
      },
      "cell_type": "markdown",
      "source": "## Summary\nFrom the above experiment, it seems that 3 pairs of convolution-subsambling is slightly better than 2 pairs. However for efficiency, the improvement doesn't warrant the additional computional cost, so let's use 2."
    },
    {
      "metadata": {
        "_uuid": "19cee794a265cc6236172017b8742a9c332f9706"
      },
      "cell_type": "markdown",
      "source": "# Experiment 2"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f137de935d61fd9a7a1e611152cda0958f251af"
      },
      "cell_type": "code",
      "source": "# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 6\nmodel = [0] *nets\nfor j in range(6):\n    model[j] = Sequential()\n    model[j].add(Conv2D(j*8+8,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n    model[j].add(MaxPool2D())\n    model[j].add(Conv2D(j*16+16,kernel_size=5,activation='relu'))\n    model[j].add(MaxPool2D())\n    model[j].add(Flatten())\n    model[j].add(Dense(256, activation='relu'))\n    model[j].add(Dense(10, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "47266925cecaada755e575fa1a072708b81094a4"
      },
      "cell_type": "code",
      "source": "# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"8 maps\",\"16 maps\",\"24 maps\",\"32 maps\",\"48 maps\",\"64 maps\"]\nepochs = 20\nfor j in range(nets):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=64, epochs = epochs, \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "15da8cf30e3329bd8bcd273b184f7da5e2495d3e"
      },
      "cell_type": "code",
      "source": "# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01c032ab2297ace6d896bd5644b1be193bc8acca"
      },
      "cell_type": "markdown",
      "source": "# Experiment 3"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7a14f4981962ebefa5bc75c1f8edea5613648d4"
      },
      "cell_type": "code",
      "source": "# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 8\nmodel = [0] *nets\n\nfor j in range(8):\n    model[j] = Sequential()\n    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n    model[j].add(MaxPool2D())\n    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n    model[j].add(MaxPool2D())\n    model[j].add(Flatten())\n    if j>0:\n        model[j].add(Dense(2**(j+4), activation='relu'))\n    model[j].add(Dense(10, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "50edbd024130a96d09aa24de6eb9bb2cf75162cd"
      },
      "cell_type": "code",
      "source": "# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"0N\",\"32N\",\"64N\",\"128N\",\"256N\",\"512N\",\"1024N\",\"2048N\"]\nepochs = 20\nfor j in range(nets):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=80, epochs = epochs, \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "23d6fcd92c5c767bf322946c08ee5cf5436e39cf"
      },
      "cell_type": "code",
      "source": "# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d2738a7fdadd227cf64d03d7c5570088778c5f8"
      },
      "cell_type": "markdown",
      "source": "# Experiment 4"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d03cf0f878e1c75338bd8620aa995c7d8048446"
      },
      "cell_type": "code",
      "source": "# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 8\nmodel = [0] *nets\n\nfor j in range(8):\n    model[j] = Sequential()\n    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n    model[j].add(MaxPool2D())\n    model[j].add(Dropout(j*0.1))\n    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n    model[j].add(MaxPool2D())\n    model[j].add(Dropout(j*0.1))\n    model[j].add(Flatten())\n    model[j].add(Dense(128, activation='relu'))\n    model[j].add(Dropout(j*0.1))\n    model[j].add(Dense(10, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "46d1ddefaf6b2bc6480136ba0840d8923bab4526"
      },
      "cell_type": "code",
      "source": "# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"D=0\",\"D=0.1\",\"D=0.2\",\"D=0.3\",\"D=0.4\",\"D=0.5\",\"D=0.6\",\"D=0.7\"]\nepochs = 30\nfor j in range(nets):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=80, epochs = epochs, \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "57790fc692a802e3b5fa1c2fc197d06590299c01"
      },
      "cell_type": "code",
      "source": "# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "fdfe0baaa31e65282ec778e5cbb0ab3c320af072"
      },
      "cell_type": "code",
      "source": "# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 5\nmodel = [0] *nets\n\nj=0\nmodel[j] = Sequential()\nmodel[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\nmodel[j].add(MaxPool2D())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Conv2D(64,kernel_size=5,activation='relu'))\nmodel[j].add(MaxPool2D())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Flatten())\nmodel[j].add(Dense(128, activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Dense(10, activation='softmax'))\nmodel[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj=1\nmodel[j] = Sequential()\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel[j].add(MaxPool2D())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(MaxPool2D())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Flatten())\nmodel[j].add(Dense(128, activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Dense(10, activation='softmax'))\nmodel[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj=2\nmodel[j] = Sequential()\nmodel[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\nmodel[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Conv2D(64,kernel_size=5,activation='relu'))\nmodel[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Flatten())\nmodel[j].add(Dense(128, activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Dense(10, activation='softmax'))\nmodel[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nj=3\nmodel[j] = Sequential()\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Flatten())\nmodel[j].add(Dense(128, activation='relu'))\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Dense(10, activation='softmax'))\nmodel[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cb6505352d68087e0c9c76e817f1d46505cf6c4"
      },
      "cell_type": "code",
      "source": "j=4\nmodel[j] = Sequential()\n\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Dropout(0.2))\n\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Dropout(0.2))\n\nmodel[j].add(Flatten())\nmodel[j].add(Dense(128, activation='relu'))\nmodel[j].add(BatchNormalization())\nmodel[j].add(Dropout(0.2))\nmodel[j].add(Dense(10, activation='softmax'))\n\nmodel[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9221a2e1e88540eef7acbe96b7b1fc4399cef4df",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.2)\n# TRAIN NETWORKS 1,2,3,4\nhistory = [0] * nets\nnames = [\"basic\",\"32C3-32C3\",\"32C5S2\",\"both+BN\",\"both+BN+DA\"]\nepochs = 35\nfor j in range(nets-1):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=64, epochs = epochs,  \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n    \n# CREATE MORE TRAINING IMAGES VIA DATA AUGMENTATION\ndatagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.1,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)\n# TRAIN NETWORK 5\nj = nets-1\nhistory[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64), \n    epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,\n    validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\nprint(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n    names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baa858ecc8022a5c08f0dd1106892b573a6ea919",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c190184093076f2c6381030e508c69fc57d2e070"
      },
      "cell_type": "markdown",
      "source": "# Predict \"test.csv\" and submit to Kaggle"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67f481323e08a7a945b9b8f3060493594cdd0be6"
      },
      "cell_type": "code",
      "source": "# TRAIN OUR BEST NET MORE\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x+epochs))\nmodel[4].fit_generator(datagen.flow(X_train,Y_train, batch_size=64), epochs = 25, \n    steps_per_epoch = X_train.shape[0]//64, callbacks=[annealer], verbose=0)\n\n# SUBMIT TO KAGGLE\nresults = model[4].predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST-CNN.csv\",index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}